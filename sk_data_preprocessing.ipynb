{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real world, the data is not perfect. You need to spend a lot of time on data preprocessing, such as cleaning, scaling, normalizing, and so on. Data preprocessing may be the most important step in the whole machine learning process. You may have heard the phrase, `\"Garbage in, garbage out\"`. If the data quality is not high, no matter how fancy the model is, an ideal result will not be achieved. Typically, for most engineers, 70 percent of the time is spent processing data.\n",
    "\n",
    "The **preprocessing** package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.\n",
    "\n",
    ">`Notice`: There are many preprocessing types and many preprocessing types. In this lesson, we will cover some most commonly used methods. If you want to learn more, just launch the `Jupyter` file at the end of this lesson. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale numerical feature\n",
    "\n",
    "Most of the time, the features in your dataset are highly varying in range. However, most of the machine learning algorithms use Euclidean distance as the metrics to measure the distance between two data points, this is a problem. So, in this case, we should make sure that the features in the same range. To solve this problem, you need to scale your data. There are many ways to scale the data.\n",
    "\n",
    "At first, let's create a 2D array for our examples. As you can see from the output below, we create a matrix(4 * 4). This dataset has four features with different ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original matrix\n",
      "[[   5    4 8997 9823]\n",
      " [   6    4 8541 1570]\n",
      " [   6    2 3740 3341]\n",
      " [   8    4 7377 8485]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.randint(2, 10, size=(4,2))\n",
    "X2 = np.random.randint(100, 10000, size=(4,2))\n",
    "X = np.concatenate((X, X2), axis=1)\n",
    "print(\"The original matrix\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax\n",
    "\n",
    "It shrinks the range such that it is now between 0 and 1\n",
    "\n",
    "$$\\frac{x_{i}-\\min (x)}{\\max (x)-\\min (x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transform data using min-max scaler\n",
      "[[0.         1.         1.         1.        ]\n",
      " [0.33333333 1.         0.91325851 0.        ]\n",
      " [0.33333333 0.         0.         0.21458863]\n",
      " [1.         1.         0.69183945 0.83787714]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing as preprocessing\n",
    "\n",
    "minmax = preprocessing.MinMaxScaler()\n",
    "minmax.fit(X)\n",
    "X_minmax = minmax.transform(X)\n",
    "print(\"The transform data using min-max scaler\")\n",
    "print(X_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard\n",
    "\n",
    "This scaler assumes that your feature is following the normal distribution. The mean and standard is calculated on the feature you want to scale.\n",
    "\n",
    "$$\\frac{x_{i}-\\operatorname{mean}(\\boldsymbol{x})}{\\operatorname{stdev}(\\boldsymbol{x})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transform data using Standard scaler\n",
      "[[-1.14707867  0.57735027  0.88859948  1.16811018]\n",
      " [-0.22941573  0.57735027  0.66757051 -1.231047  ]\n",
      " [-0.22941573 -1.73205081 -1.65953496 -0.71621514]\n",
      " [ 1.60591014  0.57735027  0.10336497  0.77915195]]\n"
     ]
    }
   ],
   "source": [
    "std = preprocessing.StandardScaler()\n",
    "std.fit(X)\n",
    "X_std = std.transform(X)\n",
    "print(\"The transform data using Standard scaler\")\n",
    "print(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust\n",
    "\n",
    "This scaler is very similar to the MinMax scaler. However, it uses the interquartile instead of the min and max. The reason is that, sometimes, there are outliers in your dataset, it will make the maximum or minimum unusually high. To eliminate such effect, a robust scaler uses the interquartile.\n",
    "\n",
    "$$\\frac{x_{i}-Q_{1}(\\boldsymbol{x})}{Q_{3}(\\boldsymbol{x})-Q_{1}(\\boldsymbol{x})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transform data using robust scaler\n",
      "[[-1.33333333  0.          0.47456852  0.66033354]\n",
      " [ 0.          0.          0.26608755 -0.73346   ]\n",
      " [ 0.         -4.         -1.92890616 -0.43436774]\n",
      " [ 2.66666667  0.         -0.26608755  0.43436774]]\n"
     ]
    }
   ],
   "source": [
    "robust = preprocessing.RobustScaler()\n",
    "robust.fit(X)\n",
    "X_robust = robust.transform(X)\n",
    "print(\"The transform data using robust scaler\")\n",
    "print(X_robust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxAbs\n",
    "\n",
    "This scaler is very similar to the MinMax scaler. The difference is that the scaler don't use the min and take the absolute value of the maximum.\n",
    "\n",
    "$$\\frac{x}{max(abs(x))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transform data using MaxAbs scaler\n",
      "[[0.625      1.         1.         1.        ]\n",
      " [0.75       1.         0.94931644 0.15982897]\n",
      " [0.75       0.5        0.41569412 0.34012013]\n",
      " [1.         1.         0.81993998 0.86378907]]\n"
     ]
    }
   ],
   "source": [
    "maxabs = preprocessing.MaxAbsScaler()\n",
    "maxabs.fit(X)\n",
    "X_maxabs = maxabs.transform(X)\n",
    "print(\"The transform data using MaxAbs scaler\")\n",
    "print(X_maxabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Normalization is also a scaler, however, it scales individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples.\n",
    "\n",
    "The function normalize provides a quick and easy way to perform this operation on a single array-like dataset, either using the **l1** or **l2** norms. The default norm is **l2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transform data using Normalizer with L2 norm\n",
      "------------------------------------------------\n",
      "[[3.75359531e-04 3.00287625e-04 6.75421940e-01 7.37431334e-01]\n",
      " [6.90917700e-04 4.60611800e-04 9.83521346e-01 1.80790132e-01]\n",
      " [1.19641800e-03 3.98805999e-04 7.45767219e-01 6.66205422e-01]\n",
      " [7.11524628e-04 3.55762314e-04 6.56114648e-01 7.54660809e-01]]\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "The transform data using Normalizer with L1 norm\n",
      "------------------------------------------------\n",
      "[[2.65547825e-04 2.12438260e-04 4.77826757e-01 5.21695257e-01]\n",
      " [5.92826796e-04 3.95217864e-04 8.43888944e-01 1.55123012e-01]\n",
      " [8.46381718e-04 2.82127239e-04 5.27577938e-01 4.71293553e-01]\n",
      " [5.03968754e-04 2.51984377e-04 4.64722187e-01 5.34521860e-01]]\n"
     ]
    }
   ],
   "source": [
    "norml2 = preprocessing.Normalizer()\n",
    "norml2.fit(X)\n",
    "X_norm = norml2.transform(X)\n",
    "print(\"The transform data using Normalizer with L2 norm\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(X_norm)\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "norml1 = preprocessing.Normalizer(norm='l1')\n",
    "norml1.fit(X)\n",
    "X_norm = norml1.transform(X)\n",
    "print(\"The transform data using Normalizer with L1 norm\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(X_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear feaure mapping for numerical feature\n",
    "\n",
    "In simple words, for some numerical features, we want to be able to do some nonlinear mapping, such as binarization according to some threshold, or data bucket according to points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarizer\n",
    "\n",
    "**Binarizer** set feature values to 0 or 1 according to a threshold. In this example, the threshold is 0.7, so the values larger than 0.7 are set 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data\n",
      "[[0.2 0.4 0.9 0.7 0.1 0.8]\n",
      " [0.8 0.1 0.2 0.8 0.1 0.4]]\n",
      "The transform data using Binarizer with threshold 0.7\n",
      "[[0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "Xb = np.array([[0.2, 0.4, 0.9, 0.7, 0.1, 0.8],\n",
    "               [0.8, 0.1, 0.2, 0.8, 0.1, 0.4]])\n",
    "binary = preprocessing.Binarizer(threshold=0.7)\n",
    "X_binary = binary.transform(Xb)\n",
    "print(\"The original data\")\n",
    "print(Xb)\n",
    "print(\"The transform data using Binarizer with threshold 0.7\")\n",
    "print(X_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile\n",
    "\n",
    "Quantile transform features using quantiles information. In this example, we pass **n_quantiles=3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data\n",
      "[[20 36 45 45]\n",
      " [ 2 29 31  7]\n",
      " [45 24 12 44]\n",
      " [33 27 39 36]]\n",
      "The transform data using QuantileTransformer with 3 quantiles\n",
      "[[0.36734694 1.         1.         1.        ]\n",
      " [0.         0.5625     0.41304348 0.        ]\n",
      " [1.         0.         0.         0.9       ]\n",
      " [0.67567568 0.375      0.7        0.43939394]]\n"
     ]
    }
   ],
   "source": [
    "Xq = np.random.randint(1, 50, size=(4, 4))\n",
    "quant = preprocessing.QuantileTransformer(n_quantiles=3)\n",
    "quant.fit(Xq)\n",
    "X_quant = quant.transform(Xq)\n",
    "print(\"The original data\")\n",
    "print(Xq)\n",
    "print(\"The transform data using QuantileTransformer with 3 quantiles\")\n",
    "print(X_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with categorical\n",
    "\n",
    "Until now, all data we are talking about is a numerical type. In fact, there's a lot of real data are categorical. Not to mention, all data in NLP are text, not numerical. In this section, we would show some methods to transform the categorical data into numerical types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoder\n",
    "\n",
    "**One-hot** is a widely used method to encode categorical data. For example, you have a feature named **size** which has tree values, \"big\", \"medium\", and \"small\". If you encode this feature by one-hot, you may get three features, \"big\", \"medium\", and \"small\". One and only one of them is assigned a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data\n",
      "[['big']\n",
      " ['medium']\n",
      " ['small']]\n",
      "The transform data using OneHotEncoder\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "feasize = np.array([[\"big\"], [\"medium\"], [\"small\"]])\n",
    "onehot = preprocessing.OneHotEncoder()\n",
    "onehot.fit(feasize)\n",
    "feasize_onehot = onehot.transform(feasize).toarray()\n",
    "print(\"The original data\")\n",
    "print(feasize)\n",
    "print(\"The transform data using OneHotEncoder\")\n",
    "print(feasize_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoder\n",
    "\n",
    "Sometimes, our label is not number, it's a string. What we will want is to convert these strings to numbers that start from 0. For binary classification, the label is 0 and 1. If the classification is three classes, then the label is 0, 1, and 2. **LabelEncoder** can help encode labels with a value between 0 and n_classes-1.\n",
    "\n",
    "As you can see from the example below, the **Sun** is encoded as **3**, the **Moon** is encoded as **2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data\n",
      "['Sun' 'Sun' 'Moon' 'Earth' 'Monn' 'Venus']\n",
      "The transform data using LabelEncoder\n",
      "[3 3 2 0 1 4]\n"
     ]
    }
   ],
   "source": [
    "targets = np.array([\"Sun\", \"Sun\", \"Moon\", \"Earth\", \"Monn\", \"Venus\"])\n",
    "labelenc = preprocessing.LabelEncoder()\n",
    "labelenc.fit(targets)\n",
    "targets_trans = labelenc.transform(targets)\n",
    "print(\"The original data\")\n",
    "print(targets)\n",
    "print(\"The transform data using LabelEncoder\")\n",
    "print(targets_trans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
